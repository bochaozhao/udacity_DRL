{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Learning Algorithm\n",
    "\n",
    "### Agent\n",
    "In this project, Deep Q network algorithm is used for training the agent. The agent contains information about the state size and number of actions to choose from. The agent also containes one local Q network and one target Q network. The Q networks are imported from the `model.py` file. The agent also contains a memory of all previous interaction with the environment which is created using the ReplayBuffer class in `dqn_agent.py`. \n",
    "\n",
    "### Deep Q network\n",
    "The deep Q network is based on Pytorch. Here we used a neural network with 2 hidden layers. The input to the network is the state variable which has dimension of state_size. The 2 hidden layers have 64 and 64 units respectively. After each hidden layer, relu is used as the activation function. The output layer has dimension of action_size and represent the Q value for choosing a specific action under a specific state. \n",
    "\n",
    "### Environment\n",
    "The environment is based on UnityEnvironment, for more description please see the `README.md` file.\n",
    "\n",
    "### Learning algorithm\n",
    "The training is perfomed in an episodic way. At the start of each episode, the environment is reset to initial state.\n",
    "\n",
    "1. The current state is passed to the local Q network and the Q value for each action is returned. Epsilon-greedy algorithm is employed to choose the action that has the maximum Q value or a random exploratory action. \n",
    "\n",
    "2. The returned action is passed to the environment, the environment determines the next state, the reward of this step and a flag indicating whether this episode is finished or not. The (state, action, reward, next state, done) tuple is called an experience and is passed to the agent and stored in the replay memory of the agent.\n",
    "\n",
    "3. After every certain number of steps, the Q network will update itself. To do this, a batch of experiences are sampled randomly from the replay memory. For each experience, a local Q value is calculated from the local network based on the state and action. A target Q value is calculated which is the sum of reward and the maximum Q value of the next state based on the target network. The mean squared error between the local and target Q value is the loss function and local network is updated to minimize the loss function. \n",
    "\n",
    "4. The target Q network is soft-updated: θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "5. The score and the current state of the current episode is updated.\n",
    "\n",
    "6. Repeat 1-5 until the episode finishes.\n",
    "\n",
    "During training, we keep track of the most recent 100 episodes and we stop training when the average score of the 100 episodes reaches 13.0\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "The buffer size for the memory replay is 1e5, batch size for a batch of experiences is 64, discount factor for the next step is 0.99, tau for soft update of target parameters is 1e-3, learning rate of the Adam optimizer is 5e-4, the network is updated every 4 steps. For epsilon-greedy algorithm, starting eps is 1.0, minimum eps is 0.01, decay factor is 0.995.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Plot of Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is solved in 422 episodes with an average score of 13.01 between episode 422 to 522. A plot of rewards is shown below "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"plot_of_rewards.png\",width=760,height=760>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some possible methods to improve the agent's performance includes: Double DQN, Prioritized Experience Replay, Dueling DQN.\n",
    "\n",
    "Double DQN:\n",
    "When using target network for calculating the maximum Q value for the next state, instead of using target network entirely, use the target network to choose the best action and then use the local network to calculate the Q value. \n",
    "\n",
    "Prioritized Experience Replay:\n",
    "Instead of complete random sampling of experiences, assign different probabilities to different experience based on the error between Q_local and Q_target. In this way, the experiences that the current Q network has a problem predicting will be sampled with a higher probabibility to facilitate learning.\n",
    "\n",
    "Dueling DQN:\n",
    "Use one network for predicting state value and another network for predicting the advantages of each action. These 2 networks may share earlier layers and only differ in the last layer. This will also faciliate learning because sometimes the state value is similar across different actions, therefore it is easier to directly estimate the state value function and then the advantage of each action. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
